{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "T"
    ]
   },
   "source": [
    "> **Student Names and IDs**:\n",
    ">\n",
    "> - Newton Kwan, nk150\n",
    "> - Joyce Choi, jc515\n",
    "> - Ashka Stephen, aas74"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "# Homework 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "## Part 1: Classifying Digit Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "NData = len(digits.images)\n",
    "data = {'x': digits.images.reshape((NData, -1)), 'y': digits.target}\n",
    "\n",
    "testFraction = 0.6\n",
    "T, S = {}, {}\n",
    "T['x'], S['x'], T['y'], S['y'] = train_test_split(data['x'], data['y'],\n",
    " test_size=testFraction, random_state=0)\n",
    "\n",
    "NT, NS = len(T['y']), len(S['y'])\n",
    "\n",
    "def evaluate(h, T, S, name):\n",
    "    def errorRate(h, S):\n",
    "        x, y = S['x'], S['y']\n",
    "        return (1 - h.score(x, y)) * 100\n",
    "    \n",
    "    f = '{:s}: training error rate is {:.2f}, test error rate is {:.2f}'\n",
    "    err = (errorRate(h, T), errorRate(h, S), name)\n",
    "    print(f.format(name, err[0], err[1]))\n",
    "    return err\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "Assuming that all digits appear with equal frequency in the data, what is the expected error rate (in percent) of a classifier that ignores the input and guesses the output uniformly and at random? Justify your answer with a very brief sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected error rate of a classifier that ignores input and guesses uniformly at random is: \n",
    "\n",
    "$$\\text{error}= 1 - (1/K)$$\n",
    "\n",
    "where K is the number of classes. Each number is guessed from a uniform probability distribution or 1/K number of times for sufficiently large number of guesses. In this specific example, the error rate is 90%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "Train a `sklearn.tree.DecisionTreeClassifier`. Show your code and report training and test error rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "def information(classifier, T, S, random_state):\n",
    "    '''\n",
    "    Desribes the result of evaluating the classifier on the sets \n",
    "    '''\n",
    "    \n",
    "    clf = classifier(random_state=random_state).fit(T['x'], T['y'])\n",
    "    train_risk = clf.score(T['x'], T['y'])\n",
    "    test_risk = clf.score(S['x'], S['y'])\n",
    "    print(\"The training error rate is {:.2f}\".format(100 - train_risk*100), \"%\")\n",
    "    print(\"The test error rate is {:.2f}\".format(100 - test_risk*100), \"%\")\n",
    "    \n",
    "    return train_risk, test_risk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training error rate is 0.00 %\n",
      "The test error rate is 20.11 %\n"
     ]
    }
   ],
   "source": [
    "d_trees = information(DecisionTreeClassifier, T, S, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "Use `GridSearchCV` to find a good number of trees for a `sklearn.ensemble.RandomForestClassifier` by 10-fold cross-validation. Show your code and report (i) the number of trees in the forest you end up choosing, (ii) the training and test error rates, and (iii) the out-of-bag error rate estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "K = 10  # K-fold cross-validation \n",
    "# create dictionary of hyper parameters \n",
    "param_grid = {} \n",
    "param_grid['random_state'] = [0]\n",
    "param_grid['n_estimators'] = range(100, 500, 100)\n",
    "\n",
    "#print(param_grid) # print out parameters as sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross validation for K = 10 \n",
    "rforest_clf = GridSearchCV(RandomForestClassifier(), param_grid, scoring = 'accuracy', cv = K)\n",
    "rforest_clf.fit(T['x'], T['y'])\n",
    "hyp_params = rforest_clf.cv_results_['params']           # list of dictionaries of hyper parameters\n",
    "test_scores = rforest_clf.cv_results_['mean_test_score'] # list of test scores (whose index corresponds to hyp_params) \n",
    "best_params = rforest_clf.best_params_                   # gives the best paramters found by GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trees in the forest: 100\n",
      "The training error rate is is 0.00 %\n",
      "The test error rate is 3.61 %\n",
      "The out-of-bag error rate is 5.29 %\n"
     ]
    }
   ],
   "source": [
    "# Retrain on T with best hyper parameters \n",
    "best_n_estimators = best_params['n_estimators'] # best hyper paramter n_estimators from clf\n",
    "new_rforest_clf = RandomForestClassifier(n_estimators = best_n_estimators, random_state=0, oob_score = True).fit(T['x'], T['y'])\n",
    "new_rforest_train_risk = new_rforest_clf.score(T['x'], T['y']) # training accuracy \n",
    "new_rforest_test_risk = new_rforest_clf.score(S['x'], S['y']) # test accuracy \n",
    "print(\"The number of trees in the forest:\", best_n_estimators)\n",
    "print(\"The training error rate is is {:.2f}\".format(100 - new_rforest_train_risk*100), \"%\")\n",
    "print(\"The test error rate is {:.2f}\".format(100 - new_rforest_test_risk*100), \"%\")\n",
    "print(\"The out-of-bag error rate is {:.2f}\".format(100 - new_rforest_clf.oob_score_*100), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "Train a `sklearn.linear_model.LogisticRegression` classifier. Show your code and report training and test error rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def information(classifier, T, S, C, solver, multi_class, random_state):\n",
    "    '''\n",
    "    Desribes the result of evaluating the classifier on the sets \n",
    "    '''\n",
    "    \n",
    "    clf = classifier(C=C, solver=solver, multi_class=multi_class, random_state=random_state).fit(T['x'], T['y'])\n",
    "    train_risk = clf.score(T['x'], T['y'])\n",
    "    test_risk = clf.score(S['x'], S['y'])\n",
    "    print(\"The training error rate is {:.2f}\".format(100 - train_risk*100), \"%\")\n",
    "    print(\"The test error rate is {:.2f}\".format(100 - test_risk*100), \"%\")\n",
    "    \n",
    "    return train_risk, test_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training error rate is 0.00 %\n",
      "The test error rate is 4.45 %\n"
     ]
    }
   ],
   "source": [
    "log_reg = information(LogisticRegression, T, S, 1e5, 'lbfgs', 'multinomial', random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "Use `GridSearchCV` to find a good number of neighbors for a `sklearn.neighbors.KNeighborsClassifier` by 10-fold cross-validation. Show your code and report (i) the number of neighbors you end up choosing, and (ii) the training and test error rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Programming Note\n",
    "\n",
    "Neighbors are in the single digits. No point in trying dozens of neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "K = 10  # K-fold cross-validation \n",
    "# create dictionary of hyper parameters \n",
    "param_grid = {} \n",
    "param_grid['n_neighbors'] = range(1, 10, 1)\n",
    "\n",
    "#print(param_grid) # print out parameters as sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross validation for K = 10 \n",
    "knn_clf = GridSearchCV(KNeighborsClassifier(), param_grid, cv = K)\n",
    "knn_clf.fit(T['x'], T['y'])\n",
    "hyp_params = knn_clf.cv_results_['params']           # list of dictionaries of hyper parameters\n",
    "test_scores = knn_clf.cv_results_['mean_test_score'] # list of test scores (whose index corresponds to hyp_params) \n",
    "best_params = knn_clf.best_params_                   # gives the best paramters found by GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of neighbors: 1\n",
      "The training error rate is is 0.00 %\n",
      "The test error rate is 1.58 %\n"
     ]
    }
   ],
   "source": [
    "# Retrain on T with best hyper parameters \n",
    "best_n_neighbors = best_params['n_neighbors'] # best hyper paramter n_estimators from knn_clf\n",
    "new_knn_clf = KNeighborsClassifier(n_neighbors = best_n_neighbors).fit(T['x'], T['y'])\n",
    "new_knn_train_risk = new_knn_clf.score(T['x'], T['y']) # training accuracy \n",
    "new_knn_test_risk = new_knn_clf.score(S['x'], S['y']) # test accuracy \n",
    "print(\"The number of neighbors:\", best_n_neighbors)\n",
    "print(\"The training error rate is is {:.2f}\".format(100 - new_knn_train_risk*100), \"%\")\n",
    "print(\"The test error rate is {:.2f}\".format(100 - new_knn_test_risk*100), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 1.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "Let us define the _overfit rate_ of a classifier as the difference between testing error and training error, both expressed as percentages. Write code that prints out the overfit rates for the tree, forest, linear classifier, and k-NN classifier, in this order, one per line. Your printout should say which is which."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree overfit rate: 20.11 %\n",
      "forest overfit rate: 3.61 %\n",
      "linear classifier overfit rate: 4.45 %\n",
      "k-NN overfit rate: 1.58 %\n"
     ]
    }
   ],
   "source": [
    "print(\"tree overfit rate: {:.2f}\".format((d_trees[0] - d_trees[1])*100), \"%\")\n",
    "print(\"forest overfit rate: {:.2f}\".format((new_rforest_train_risk - new_rforest_test_risk)*100), \"%\")\n",
    "print(\"linear classifier overfit rate: {:.2f}\".format((log_reg[0] - log_reg[1])*100), \"%\")\n",
    "print(\"k-NN overfit rate: {:.2f}\".format((new_knn_train_risk - new_knn_test_risk)*100), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code that prints out the overfit rates for the tree, forest, linear classifier, and k-NN classifier, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 1.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "Discuss briefly the results you obtained, by answering the following questions:\n",
    "- Which classifier works best on this data set?\n",
    "- Why doesn't everyone always use the best-performing classifier for a problem like this?\n",
    "- Do random forests reduce overfitting when compared with decision trees?\n",
    "- In what ways is a linear classifier more convenient than algorithms that may perform better, when optimal performance is not crucial?\n",
    "- For all but one of the classifiers used, a zero training error rate is no surprise. Which is the odd-man-out, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The k-NN classifier works the best with test error rate of 1.58%. \n",
    "- We don’t always use the best-performing classifier because it depends on the dataset. Sometimes the best performing classifier might not generalize well enough. It could also be because we want to trade-off accuracy for time to have a classifier that’s faster and “good enough”.\n",
    "- Yes, random forest reduce overfitting when compared with decision trees. We see a reduction in the overfit rate from 20.11% to 3.61% \n",
    "- Linear classifiers generalize well to other problems, which might be preferable to having a very specific, complex model. It also could have faster calculations than a more complex classifier.\n",
    "- We expect 0% for decision trees, k-nearest, and random forests. Therefore we are surprised that logistic regression is able to achieve 0% error because that means the decision region is convex (aka the data is linearly separable). This does not happen too often (dataset dependent), especially as we get into higher dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "## Part 2: Closed Convex Polyhedral Cones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "Write a function with header\n",
    "    `def parametric(a):`\n",
    "that takes a list `a` with the vectors of an implicit representation of a CCP cone on the plane and returns a list with the generators of the parametric representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "T"
    ]
   },
   "source": [
    "Show your code in a new code cell under the _Answer_ header below. The tests after that will draw your results if your code has no bugs. Of course, you need to make sure that the drawings make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def parametric(l):\n",
    "    # edge 1: check if superimposed or only one vector \n",
    "    if (len(l)==1) or (checkNormal(l)):\n",
    "        for eachPoint in l:\n",
    "            generated = generatePoints(eachPoint[0],eachPoint[1])\n",
    "        generated.append(l[0])\n",
    "        # print(\" - final result: \", removeDuplicates(generated))\n",
    "        return (removeDuplicates(generated))\n",
    " \n",
    "    #edge 2: check if pointing opposite\n",
    "    opposite = list([-l[0][0], -l[0][1]])\n",
    "    if (checkNormal([opposite,l[1]])):\n",
    "        for eachPoint in l:\n",
    "            generated = generatePoints(eachPoint[0],eachPoint[1])\n",
    "        # print(\" - final result: \", removeDuplicates(generated))\n",
    "        return (removeDuplicates(generated))\n",
    "\n",
    "    saved = []\n",
    "    for eachPoint in l:\n",
    "        generated = generatePoints(eachPoint[0],eachPoint[1])\n",
    "        for eachR in generated:\n",
    "            if (checkDotProd(eachR,l)):\n",
    "                saved.append(eachR)\n",
    "    # print(\" - final result: \", removeDuplicates(saved))\n",
    "    return saved\n",
    "  \n",
    "\n",
    "def removeDuplicates(l):\n",
    "    final_list = [] \n",
    "    for point in l: \n",
    "        if point not in final_list: \n",
    "            final_list.append(point) \n",
    "    return final_list \n",
    "\n",
    "'''\n",
    "check if two vectors have the same normal\n",
    "'''\n",
    "def checkNormal(l):\n",
    "    list0 = list(l[0])\n",
    "    list1 = list(l[1])\n",
    "    \n",
    "    len0 = math.sqrt(list0[0]**2 + list0[1]**2)\n",
    "    len1 = math.sqrt(list1[0]**2 + list1[1]**2)\n",
    "    \n",
    "    newList0 = [x / len0 for x in list0]\n",
    "    newList1 = [x / len1 for x in list1]\n",
    "    \n",
    "    return (newList0 == newList1)\n",
    "\n",
    "'''\n",
    "  given an x and y - output list of orthogonal\n",
    "'''\n",
    "def generatePoints(x,y):\n",
    "    return [(-y,x),(y,-x)] \n",
    "    \n",
    "'''\n",
    "output the list of points that satisfy all constraints w dot product check\n",
    "given an r vector (testR) and a list of all a's to compare (l)\n",
    "'''\n",
    "def checkDotProd(testR, l):\n",
    "    for each in l:\n",
    "        if np.dot(testR, each) < 0: \n",
    "            # should not happen \n",
    "            return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "implicit:{(0, 1), (1, 0)}; parametric: {(1, 0), (0, 1)}\n",
      "implicit:{(1, 2), (2, 1)}; parametric: {(2, -1), (-1, 2)}\n",
      "implicit:{(1, 2), (-1, 0)}; parametric: {(-2, 1), (0, 1)}\n",
      "implicit:{(1, 1), (-1, -1)}; parametric: {(1, -1), (-1, 1)}\n",
      "implicit:{(0, 1)}; parametric: {(-1, 0), (1, 0), (0, 1)}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEACAYAAABWLgY0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113d6dbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Tests; Your code should be in a separate code cell above this one\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "tests = [[(0, 1), (1, 0)],\n",
    "         [(1, 2), (2, 1)],\n",
    "         [(1, 2), (-1, 0)],\n",
    "         [(1, 1), (-1, -1)],\n",
    "         [(0, 1)]]\n",
    "\n",
    "def vectorString(a):\n",
    "    return '{' + ', '.join(['(' + ', '.join([str(b[i]) \\\n",
    "        for i in range(len(b))]) + ')' for b in a]) + '}'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def draw(a, p, k):\n",
    "    plt.subplot(2, 3, k)\n",
    "    for b in a:\n",
    "        plt.quiver(0, 0, b[0], b[1], color='r', units='width', scale=5)    \n",
    "    for q in p:\n",
    "        plt.quiver(0, 0, q[0], q[1], color='b', units='width', scale=5)\n",
    "    plt.axis('equal')\n",
    "    plt.axis('off')\n",
    "    plt.xlim([-1, 1])\n",
    "    plt.ylim([-1, 1])\n",
    "\n",
    "try:\n",
    "    plt.figure()\n",
    "    k = 1\n",
    "    for a in tests:\n",
    "        p = parametric(a)\n",
    "        print('implicit:', vectorString(a), '; parametric: ', vectorString(p), sep='')\n",
    "        draw(a, p, k)\n",
    "        k += 1\n",
    "    plt.show()\n",
    "except NameError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "Write a function with header\n",
    "    `def implicit(p):`\n",
    "that takes a list `p` with the generators of a parametric representation of a CCP cone on the plane and returns a list with the vectors for the implicit representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "T"
    ]
   },
   "source": [
    "Show your code in a code cell under the _Answer_ header below. The tests after that will draw your results if your code has no bugs. Of course, you need to make sure that the drawings make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implicit(l):\n",
    "    # edge 1: check if superimposed or only one vector \n",
    "    if (len(l)==1) or (checkNormal(l)):\n",
    "        for eachPoint in l:\n",
    "            generated = generatePoints(eachPoint[0],eachPoint[1])\n",
    "        generated.append(l[0])\n",
    "        # print(\" - final result: \", removeDuplicates(generated))\n",
    "        return (removeDuplicates(generated))\n",
    " \n",
    "    #edge 2: check if pointing opposite\n",
    "    opposite = list([-l[0][0], -l[0][1]])\n",
    "    if (checkNormal([opposite,l[1]])):\n",
    "        for eachPoint in l:\n",
    "            generated = generatePoints(eachPoint[0],eachPoint[1])\n",
    "        # print(\" - final result: \", removeDuplicates(generated))\n",
    "        return (removeDuplicates(generated))\n",
    "\n",
    "    saved = []\n",
    "    for eachPoint in l:\n",
    "        generated = generatePoints(eachPoint[0],eachPoint[1])\n",
    "        for eachR in generated:\n",
    "            if (checkDotProd(eachR,l)):\n",
    "                saved.append(eachR)\n",
    "    # print(\" - final result: \", removeDuplicates(saved))\n",
    "    return saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parametric: {(0, 1), (1, 0)}; implicit:{(1, 0), (0, 1)}\n",
      "parametric: {(1, 2), (2, 1)}; implicit:{(2, -1), (-1, 2)}\n",
      "parametric: {(1, 2), (-1, 0)}; implicit:{(-2, 1), (0, 1)}\n",
      "parametric: {(1, 1), (-1, -1)}; implicit:{(1, -1), (-1, 1)}\n",
      "parametric: {(0, 1)}; implicit:{(-1, 0), (1, 0), (0, 1)}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEACAYAAABWLgY0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAC2xJREFUeJzt3V+I5eddx/HPs0lcZCqGsptiDMYm8XLTgkSU3rShpe2F\nWgoKQjetjRANFGKt8UK3Id2AsFQsFBoLJtJsof65MoIalLY3Qs1SqYkgSEyzEis2sbTYFULJPl7M\nJLvZnZ0558w55/md77xel9nZmS/zI+/98uzs72m99wBQ05HRAwCwOiIPUJjIAxQm8gCFiTxAYSIP\nUJjIAxQm8gCFiTxAYSIPUJjIAxQm8gCFiTxAYSIPUJjIAxQm8gCFiTxAYSIPUJjIAxQm8gCFiTxA\nYSIPUJjIU1drN6W1nx09xm5ay8+1lptGz8FyTfG5ijyVvZzkL9La20YPcrnW8rYkf5bt+Shiqs9V\n5Kmr94tJvpXkybR2y+hxkqS13JLkr5J8q/dcHD0PyzHl5yryVPdokv9M8tLoQXa8lO15Hh09CEs1\n2efaeu+jZ4DVau3+JD29D/8fsLXcnyS953OjZ2F5pvxcRZ76Wjua5CtJ3pXeXxk3Ro4m+WqSd/ae\nYXOwXFN/ro5rqG877F9M8tHBk9yb5OwUQ8CBTPq52uQ5HAZv81Pf9ljMJjxXmzyHw/htftLbHgub\n/HO1yXN4DNrmN2HbY36b8lxt8hwe47b5yW97LGQjnqtNnsNlzdv8pmx7zGeTnqtNnsNl/dv8Rmx7\nzG1jnqtNnsNnTdv8Jm17zG7TnqtNnsNnfdv8xmx7zGWjnqtNnsNpxdv8pm17zGYTn6tNnsNp9dv8\nRm17zGzjnqtNnsNrRdv8Jm577G9Tn6tNnsNrddv8xm17zGQjn6vIr8BD7eGjo2dgZo8lObmz1R/Y\nzrZ3cufzrkVrOTK1K+eusn0V4zR709r1+3/I+p/rskzzm77hvpm3PjF6Bma0/G1+bdtea/lIa/nH\nJC8kuW3VX++AbkvyQlr7Wlr7yOhhrnDfDB+zkVt84kx+6U6108c/m4/998fy2bec7qemchsRe1nS\n2fy6zmx3vs69ST6U5MeT/HzveWZVX29ptu/afTLbNyh9McljI9/vvzPTVpIXk9yS3i/s/iGbeRb/\nGpv8kr2U4x//Xm5sL+fYA6NnYUbL2+ZXuu21lqM7NxB9JUlP8q4k79iIwCdJ7/+c5B3ZnjtJvprW\n7l/WUdmC3pPkxiTv3uNjNnaLT0R+6d6U7587kWcubOXC10fPwlwOdDa/yjPb3eLeex7tPa/0nheX\n/fVWqvcX0/sr6f1zSd65819Hxv7fkzyd5PndfnGTz+Jf47hmBe5q575zrt/15tFzMKcD3AW7ijs+\nrziWOZvk8U3dJve0Hfd7sx3Ts1n3MU5rX07vd+/+S9O9u3VWNnm4ZKFtftnb3l6b+zI+/+RMa7N/\nXYUtPhF5uGTxs/mlnNkeurhfaXqx3+iz+NeIPLzRXNv8Mra9Qx/3K00g9lW2+ETk4Y3m3+YX3vbE\nfR9jY19ii09EHnYz0za/6LYn7nNac+wrbfGJyMPVZt/m59r2xP2A1hf7Mlt8IvJwLXtu8/Nse+K+\nZCuMfbUtPhF52N3+2/y+2564r9hqYl9qi09EHvay6za/37Yn7mu2pNhX3OITkYdru/Y2v+u2J+6D\nHTz25bb4RORhP2/Y5nfb9sR9YhaIfdUtPhF52NvV2/zr2564T9x8sS+5xSciD7N4LMnJT7ZP/Uh2\nXqIl7htkn9j/X364pegWn4j8JH2iffqDv97+6PdPtdPHR89CXt/mn8vtf5zkfJKnIu7DtZat1vKB\n1nJipt9wjdj/QX7r5hTd4hOvGl6J97W/+df/ybG3LPr7X8nRH3o2d279aL7bfzF/+ec/mRdOPtwf\n+sEyZ2ROrR19T576l5vy7X+6Pc//2qf6J/939EiHVWu5PttX9j2S7Qs/nk6y661OezmSV9tP5D9u\n/qn8W/4u771T5FmbT7RPf/BCtn76WF7+jCsEJ2T0e895g9ayle0bnZ7vPc+OnmeqRB7mJfZsEGfy\nMK8JvAoXZiXysCixZwOIPByU2DNhIg/LIvZMkMjDsok9EyLysCpizwSIPKya2DOQyMO6iD0DiDys\nm9izRiIPo4g9ayDyMJrYs0IiD1Mh9qyAyMPUiD1LJPIwVWLPEog8TN1esW/tloGTsQFEHjbFbrFP\n/iGt3TlsJibPpSGwqVr7cJLfSHJzkl9O718bPBETJPKw6Vo7kuRYev/26FGYHpEHKMyZPEBhIg9Q\nmMgDFCbyAIWJPEBhIg9QmMgDFCbyAIWJPEBhIg9QmMgDFCbyAIWJPEBhIg9QmMgDFCbyAIWJPEBh\nIg9QmMgDFCbyAIWJPEBhIg9QmMgDFCbyAIWJPEBhIg9QmMgDFCbyAIWJPEBhIg9QmMgDFCbyAIWJ\nPEBhIg9QmMgDFCbyAIWJPEBhIg9QmMgDFCbyAIWJPEBhIg9QmMgDFCbyAIWJPEBhIg9QmMgDFCby\nAIWJPEBhIg9QmMgDFCbyAIWJPEBhIg9QmMgDFCbyAIWJPEBhIg9QmMgDFCbyAIWJPEBhIg9QmMgD\nFCbyAIWJPEBhIg9QmMgDFCbyAIWJPEBhIg9QmMgDFCbyAIWJPEBhIg9QmMgDFCbyAIWJPEBhIg9Q\nmMgDFCbyAIWJfJK0dkNam+T3orVc11puGD0HyUPt4aOjZ4B5TTJsA1xM8idp7f1prY0eJklaS2st\n70/yeJJXR89D8s289YnRM8C8RD5Jen81yZ8m+eskX0hrQzfnnc39CzvzfKn3XBw5D8mpdvr4k/mF\nXzrVTh8fPQvMQ+Qv+dskZ5PckeTWwbPcujPH2SRPDZ6FJC/l+Me/lxvbyzn2wOhZYB7Xjx5gMnrv\nae3ebAf2ibR2T3p/bt1jtJY7kjyR5J4k53tPX/cMXO1N+f65E3nmwlYufH30LDCP1ruGXKW1S6Fd\nY+gvD3zvWfsfMOztrnbuO+f6XW8ePQfMw3HNbrbDfk+2N/o71vElBR5YBZG/ljWGXuCBVRH5vawh\n9AIPrJLI72eFoRd4YNVEfhYrCL3AA+sg8rNaYugFHlgXkZ/HEkIv8MA6ify8DhB6gQfWTeQXsUDo\nBR4YQeQXNUfoBR4YReQPYobQCzwwksgf1B6hF3hgNJFfhl1CL/DAFIj8slwW+gfbmbsj8NPR2lZa\n+0BaOzF4jhM7c2wNnYNDxauGl+zBdubub+Ttn397vnHfmf7gl0fPc6i1dn2S+5I8kuTGJE8nubDo\np/vDPPBjv5nP/NcBJtpK8jNJvpvkd5N8fudWMlgZkV+i145ofjtnfu9MfueRrPl99FzD9ub87iTP\np/dnB85xIsltSf4+vS/8hw3MQ+SX5Koz+EEXjwBczpn8Euz6l6wDLh4BuJLIH9CeP0Uj9MBgIn8A\nM/2YpNADA4n8gub6OXihBwYR+QUs9A+dhB4YQOTndKB/ySr0wJqJ/ByW8qoCoQfWSORntNR30Qg9\nsCYiP4OVvGxM6IE1EPl9rPRtkkIPrJjI72EtrwsWemCFRP4a1vo+eKEHVkTkdzHkwg+hB1bAWygv\n01puSHJrRl74cfnbK5Pz6f0Ha58BKEPkd7SWlu243p7RNzpdCv1zST4cDwlYkOOaS96X5EPZDuv5\nwbOc35njZJL3Dp4F2GA2+SSt5bokjyf5UpKnes/4b0prLduB/5Ukv5reLw6eCNhAIp/Xz+Jf7T3T\nC2lrR5Jc52weWITIAxTmTB6gMJEHKEzkAQoTeYDCRB6gMJEHKEzkAQoTeYDCRB6gMJEHKEzkAQoT\neYDCRB6gMJEHKEzkAQoTeYDCRB6gMJEHKEzkAQoTeYDCRB6gMJEHKEzkAQoTeYDCRB6gMJEHKEzk\nAQoTeYDCRB6gMJEHKEzkAQoTeYDCRB6gMJEHKEzkAQoTeYDCRB6gMJEHKEzkAQoTeYDCRB6gMJEH\nKEzkAQoTeYDCRB6gMJEHKEzkAQoTeYDCRB6gMJEHKOz/AXm3+IZp/R0YAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d1470f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Tests; Your code should be in a separate code cell above this one\n",
    "\n",
    "try:\n",
    "    plt.figure()\n",
    "    k = 1\n",
    "    for p in tests:\n",
    "        a = implicit(p)\n",
    "        print('parametric: ', vectorString(p), '; implicit:', vectorString(a), sep='')\n",
    "        draw(a, p, k)\n",
    "        k += 1\n",
    "    plt.show()\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "Let\n",
    "\n",
    "$$ P = \\{\\mathbf{u}\\in\\mathbb{R}^2\\ :\\ \\mathbf{a}_1^T\\mathbf{u} \\geq 0 \\;\\;\\text{and}\\;\\;\n",
    "\\mathbf{a}_2^T\\mathbf{u} \\geq 0\\ \\}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\mathbf{a}_1^T = (0, 1) \\;\\;\\;\\text{and}\\;\\;\\; \\mathbf{a}_2^T = (-2, 1)\\;. $$\n",
    "\n",
    "What is the _implicit_ (not parametric!) representation of the dual $D$ of $P$?\n",
    "\n",
    "_Hint:_ The previous problems may help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The implicit representation of the dual D of P is a1 = (-1, 0) and a2 = (1, 2)\n"
     ]
    }
   ],
   "source": [
    "answer = implicit([(0, 1), (-2, 1)])\n",
    "print(\"The implicit representation of the dual D of P is a1 =\",answer[0], \"and a2 =\",answer[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "## Part 3: Convex Programs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "Use the definition of function convexity to prove that the function\n",
    "\n",
    "$$ f(u, v) = u + v $$\n",
    "\n",
    "is (weakly) convex in the vector $\\mathbf{u} = (u, v)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f$ is convex is for every $\\mathbf{u}, \\mathbf{v} \\in R^m$ and $t \\in [0,1]$ the following inequality holds \n",
    "\n",
    "$$ f(t\\mathbf{u} + (1-t)\\mathbf{v}) \\leq tf(\\mathbf{u}) + (1-t)f(\\mathbf{v})$$\n",
    "\n",
    "Let $\\mathbf{u} = (u, v)$ and $\\mathbf{v} = (u_2, v_2)$. Showing that the left side of the inequality is equivalent to the right side of the inequality will be sufficient proof that $f$ is weakly convex.\n",
    "\n",
    "Starting with the left side of the inequality, \n",
    "\n",
    "$$f(t\\mathbf{u} + (1-t)\\mathbf{v}) = f(t\\begin{bmatrix} u \\\\ v \\end{bmatrix} + (1-t)\\begin{bmatrix} u_2 \\\\ v_2 \\end{bmatrix}) = f(\\begin{bmatrix} tu + u_2 - tu_2 \\\\ tv + v_2 - tv_2 \\end{bmatrix}) = t(u - u_2 + v - v_2) + u_2 + v_2$$\n",
    "\n",
    "Now expanding the right side of the inequality, \n",
    "\n",
    "$$tf(\\mathbf{u}) + (1-t)f(\\mathbf{v}) = t(u + v) + (1-t)(u_2+v_2) = tu - tu_2 + tv  - tv_2 +u_2+v_2 = t(u + - u_2 + v  - v_2) + u_2 + v_2$$ \n",
    "\n",
    "Both sides of the inequality are equal, which is sufficient to prove that $f$ is weakly convex. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "Use the KKT conditions (1) and (2) to write expressions for $\\alpha_1$ and $\\alpha_2$ in terms of $u_1$, $a_1$, $u_2$, $a_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\alpha_1 = 2(u_1 - a_1)$$\n",
    "$$\\alpha_2 = 2(u_2 - a_2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "For $\\mathbf{a} = \\mathbf{a}_1$ (given in the preamble), compute the values of the Lagrange multipliers $\\alpha_1$ and $\\alpha_2$ for each of the four test points using the expressions you found in Problem 3.2, and check which of the test points, if any, satisfies the remaining KKT conditions (5), (6), (7). Based on this check, is any of the four test points a solution to the convex program? If so, which?\n",
    "\n",
    "Your answer should just\n",
    "- state which of the four points solve(s) the convex program,\n",
    "- list the corresponding values of $\\alpha_1$ and $\\alpha_2$, and\n",
    "- give the active set $\\mathcal{A}$ for the solution.\n",
    "\n",
    "If there is no solution, so state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\mathbf{v}_4$ solves the convex program\n",
    "- $\\alpha_1 = 0$; $\\alpha_2 = 0$\n",
    "- $A = \\{\\}$ is the active set for the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "Same as Problem 3.3, but for $\\mathbf{a} = \\mathbf{a}_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\mathbf{v}_2$ solves the convex program \n",
    "- $\\alpha_1 = 2$; $\\alpha_2 = 0$\n",
    "- $A = \\{1\\}$ is the active set for the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "Same as Problem 3.3, but for $\\mathbf{a} = \\mathbf{a}_3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\mathbf{v}_1$ solves the convex program \n",
    "- $\\alpha_1 = 2$; $\\alpha_2 = 4$\n",
    "- $A = \\{1,2\\}$ is the active set for the solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
